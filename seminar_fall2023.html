<head>
  <title>HDSC Seminar, Fall 2023</title>
</head>

<body>

  <div style="max-width:700px; word-wrap:break-word; margin-left:25px">

    <p>

      <font face="Georgia">


	<h1><br>HDSC Seminar, Fall 2023</h1>
	<i>Organizer</i>: Michael Lindsey<br>
	<i>Meeting time</i>: Tuesday 11-12 (location TBA) <br><br>

	<h2>Description</h2>

	Welcome to an informal seminar on <b>high-dimensional scientific computing (HDSC)</b>. We will investigate paradigms for HDSC 
	including tensor networks, Monte Carlo methods, semidefinite programming relaxations, graphical models, neural networks, and more, as well as tools from numerical 
	linear algebra and optimization.<br><br>

	<h2>Schedule</h2>
	<i>Click for abstracts.</i><br><br>

    <details><summary> <b>September 5</b> <br> <i>Speaker</i>: Michael Lindsey <a href="https://quantumtative.github.io/">[home page]</a> <br> <i>Topic</i>: <b>Introduction to tensor networks</b>  </summary>
		<p style="margin-left:30px; font-size:14px">
		<b><i>Abstract.</i></b> TBA.
		</p>
		</details>
		<br>

	<details><summary><b>September 12</b> <br> <i>Speaker</i>: Vivek Bharadwaj <a href="https://vbharadwaj-bk.github.io/">[home page]</a> <br> <i>Topic</i>: <b>TBA</b>  </summary>
		<p style="margin-left:30px; font-size:14px">
		<b><i>Abstract.</i></b> TBA.
		</p>
		</details>
		<br>

	<h2>Sample topics to present</h2>
	<i>In no particular order.</i> <br>
	<ol>

		<li> <a href="https://arxiv.org/abs/2207.02149">Stochastic Optimal Control for CV Free Sampling of Molecular Transition Paths</a>
		</li> <br>

		<!-- <li>Hierarchical matrices for GPR:
			<ol type="a">
				<li><a href="https://arxiv.org/pdf/1403.6015.pdf">Fast Direct Methods for Gaussian Processes</a></li>
				<li><a href="https://arxiv.org/abs/1405.0223">Fast symmetric factorization of hierarchical matrices with applications</li>
			</ol>

		</li> <br> -->

		<li><a href="https://arxiv.org/abs/2210.10210">Equispaced Fourier representations for efficient GPR</a></li><br>

		<li>Structured matrix recovery from matvecs: 
			<a href="http://dx.doi.org/10.1016/j.jcp.2011.02.033">Lin et al</a>, 
			<a href="https://arxiv.org/pdf/2212.09841.pdf">Halikias and Townsend</a> </li><br>

		<li><a href="https://openreview.net/pdf?id=ACThGJBOctg">Kernel Interpolation with Sparse Grids</a></li><br>

		<li><a href="https://doi.org/10.1137/140978430">A DEIM Induced CUR Factorization</a></li>
			<ul>
				<li>See also the references and discussion in <a href="https://personal.math.vt.edu/embree/cur_talk.pdf">these slides</a></li>
			</ul>
			<br>
		
		<li>
			<a href="https://arxiv.org/abs/2207.06503">Randomly pivoted Cholesky</a> and <a href="https://arxiv.org/abs/2301.07825">XTrace</a>
		</li><br>

		<li>Belief propagation background</li>
			<ol type="a">
				<li><a href="https://people.eecs.berkeley.edu/~wainwrig/Papers/WaiJor08_FTML.pdf">Wainwright and Jordan</a></li>
				<li><a href="https://web.stanford.edu/~montanar/RESEARCH/book.html">M&eacute;zard and Montanari</a></li>
			</ol>
			<br>

		<li>Belief propagation for tensor networks</li>
			<ol type="a">
				<li><a href="https://arxiv.org/abs/1710.01437">Duality of Graphical Models and Tensor Networks</a></li>
				<li><a href="https://arxiv.org/abs/2301.05844">Block Belief Propagation Algorithm for 2D Tensor Networks</a></li>
				<li><a href="https://arxiv.org/abs/2306.17837">Gauging tensor networks with belief propagation</a></li>
			</ol>
			<br>

		<li>General tensor network contraction</li>
			<ol type="a">
				<li><a href="https://doi.org/10.22331/q-2021-03-15-410">Hyper-optimized tensor network contraction</a></li>
				<li><a href="https://arxiv.org/abs/2206.07044">Hyper-optimized compressed contraction of tensor networks with arbitrary geometry</a></li>
				<li><a href="https://arxiv.org/abs/1912.03014">Contracting Arbitrary Tensor Networks</a></li>
			</ol>
			<br>

		<li><a href="https://doi.org/10.1103/PhysRevResearch.5.013156">Arithmetic circuit tensor networks</a></li><br>

		<li><a href="https://arxiv.org/abs/1511.06029">QTT for integral equations</a></li><br>

		<li>Neural networks and Gaussian processes</li>
			<ol type="a">
				<li><a href="https://arxiv.org/abs/1711.00165">Deep Neural Networks as Gaussian Processes</a></li>
				<li><a href="https://arxiv.org/pdf/1806.07572.pdf">Neural tangent kernel</a></li>
				<li><a href="https://arxiv.org/pdf/2206.07673.pdf">Wide Bayesian neural networks have a simple weight posterior</a></li>
			</ol>
			<br>

		<li>Generative modeling</li>
			<ol type="a">
				<li><a href="https://arxiv.org/abs/2209.15571">Building Normalizing Flows with Stochastic Interpolants</a></li>
				<li><a href="https://arxiv.org/abs/2303.08797">Stochastic Interpolants: A Unifying Framework for Flows and Diffusions</a></li>
				<li><a href="https://arxiv.org/abs/1811.00995">Invertible Residual Networks</a></li>
				<li><a href="https://arxiv.org/abs/2012.05942">Convex Potential Flows</a></li>
			</ol>
			<br>

		<li>Crash course in pseudospectral methods, i.e., Ch. 4-5 of <a href="https://depts.washington.edu/ph506/Boyd.pdf">Boyd's book</a></li><br>
		
		<li>Adaptive multigrid</li>
			<ol type="a">
				<li><a href="https://www.math.mun.ca/~smaclachlan/research/aSA2.pdf">Adaptive smoothed aggregation</a></li>
				<li><a href="https://grandmaster.colorado.edu/~stevem/pubs/fosls-mg/Multigrid_Publications_files/aamg1.pdf">Adaptive algebraic multigrid</a></li>
				<li><a href="https://doi.org/10.1016/j.jcp.2020.109356">Multigrid deflation for trace estimation</a></li>
			</ol>
			<br>

		<li>Sampling for lattice gauge theories, cf. <a href="https://link.springer.com/book/10.1007/978-3-642-01850-3">Gattringer and Lang</a> and talk to me</li><br>
		
		<li>Complex Langevin, cf. <a href="https://arxiv.org/abs/1907.10183">this review</a> and <a href="https://arxiv.org/abs/1512.05145">Sec. 8 of this review</a></li>
	</ol>

</body>
